<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Anti GFW guide]]></title>
    <url>%2F2018%2F07%2F17%2FAnti-GFW-guide%2F</url>
    <content type="text"><![CDATA[公司内网环境有美国代理，但Wi-Fi/4G环境是国内网络，使用Mac与手机时诸多不便，科学上网简要步骤记录如下 ss服务器购买vps或者ecs，国内外提供商都有，网上随便搜搜，各大云厂商有折扣就买，一个月几十块。我之前用的是华为云香港区的ecs，选centos系统，KeyPair登陆，绑上弹性IP，安全组开放TCP入方向443端口。1234easy_install pipyum install gitpip install git+https://github.com/shadowsocks/shadowsocks.git@masterssserver -p 443 -k yourpswd -m aes-256-cfb --user nobody -d start vultr有25+3=28刀优惠，最便宜的ipv4机器每个月3.5刀，相当于免费用8个月，注册链接https://www.vultr.com/promo25b?service=promo25b 123yum install net-toolswget -N --no-check certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; ./ssr.shwget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh &amp;&amp; chmod +x bbr.sh &amp;&amp; ./bbr.sh ss客户端 iPhone我手机没越狱，在闲鱼买了个香港或者美国appleid，下载shadowrocket、kite、openwingy等软件，配置服务器ip密码加密方式即可 MacBook 网页shadowsocks官网下载客户端（需先用vpn软件翻墙）shadowsocksX/shadowsocksX-NG，安装后配置与iPhone类似 MacBook 终端终端默认无法使用http代理，使用brew、sbt、git等命令时特别慢，需要安装privoxy（shadowsocksX-NG自带），在shell的rc脚本中配置（~/.zshrc) 123alias proxy=&apos;export all_proxy=socks5://127.0.0.1:1080&apos;alias unproxy=&apos;unset all_proxy&apos;export JAVA_OPTS=&quot;$JAVA_OPTS -Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=8118&quot;]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>GFW</tag>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mesos net_cls test]]></title>
    <url>%2F2017%2F05%2F27%2Fmesos-net-cls-test%2F</url>
    <content type="text"><![CDATA[1.mesos启动参数配置slave启动参数配置，增加net_cls 1echo cgroups/cpu,cgroups/mem,cgroups/net_cls &gt; /etc/mesos-slave/isolation 增加mesos-slave启动参数--cgroups_net_cls_primary_handle=0x0001，可以通过修改/usr/bin/mesos-init-wrapper脚本实现，最终mesos-slave启动参数如下： 1/usr/sbin/mesos-slave --master=zk://10.120.177.85:2181,10.120.181.94:2181,10.120.180.209:2181/mesos --log_dir=/var/log/mesos --cgroups_net_cls_primary_handle=0x0001 --containerizers=mesos --isolation=cgroups/cpu,cgroups/mem,cgroups/net_cls --work_dir=/var/lib/meso 此时mesos会为每个容器分配一个16位的cgroups_net_cls_secondary_handle，和cgroups_net_cls_primary_handle(0x0001)一起组成一个classid。 2. 启动应用使用marathon提交nc命令行应用，向对端发数据： 12345678&#123; "id": "/basic-023", "cmd": "yes ssssssssssssssssssssss|nc 10.162.174.188 5567", "cpus": 1, "mem": 10, "disk": 0, "instances": 1&#125; 使用nethogs工具查看进程流量： 3. 启用流控规则查看容器进程的classid： 65538 = 0x00010002 使用tc工具配置规则： 12345tc qdisc del dev eth0 roottc qdisc add dev eth0 root handle 1: htbtc class add dev eth0 parent 1: classid 1: htb rate 1000mbit ceil 1000mbittc class add dev eth0 parent 1: classid 1:2 htb rate 1mbittc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 1:2 cgroup 查看进行流控后的容器进程流量：]]></content>
      <categories>
        <category>Mesos</category>
      </categories>
      <tags>
        <tag>Mesos</tag>
        <tag>net_cls</tag>
        <tag>流控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Get Started with Flink on top of Mesos Marathon]]></title>
    <url>%2F2017%2F04%2F24%2Fget-started-with-flink-on-top-of-mesos-marathon%2F</url>
    <content type="text"><![CDATA[Based on CentOS 7 准备rpm安装包准备zookeeper mesos marathon 环境准备三台机器的集群，网络互通，关闭防火墙和SELinux 1systemctl stop firewalld &amp;&amp; setenforce 0 节点 IP 部署服务 1 10.120.177.85 zookeeper、mesos-master、mesos-slave、marathon 2 10.120.181.94 zookeeper、mesos-master、mesos-slave、marathon 3 10.120.180.209 zookeeper、mesos-master、mesos-slave、marathon 安装node1、node2、node3: 1234rpm -ivh zookeeper*.rpmyum install libevent libevent-devel -yrpm -ivh mesos*.rpmrpm -ivh marathon*.rpm 配置zookeeper配置node1: 1echo 1 &gt; /var/lib/zookeeper/myid node2: 1echo 2 &gt; /var/lib/zookeeper/myid node3: 1echo 3 &gt; /var/lib/zookeeper/myid node1、node2、node3配置/etc/zookeeper/zoo.cfg 123server.1=10.120.177.85:2888:3888server.2=10.120.181.94:2888:3888server.3=10.120.180.209:2888:3888 mesos配置node1、node2、node3配置/etc/mesos/zk 1zk://10.120.177.85:2181,10.120.181.94:2181,10.120.180.209:2181/mesos node1、node2、node3: 12#集群中节点数为3echo 2 &gt; /etc/mesos-master/quorum marathon配置node1、node2、node3: 12#用于生成marathon.jarmarathon 启动node1、node2、node3: 1234systemctl restart zookeepersystemctl restart mesos-mastersystemctl restart mesos-slavemarathon run_jar --master zk://10.120.177.85:2181,10.120.181.94:2181,10.120.180.209:2181/mesos --zk zk://10.120.177.85:2181,10.120.181.94:2181,10.120.180.209:2181/marathon 可通过三个节点IP访问mesos webUI和marathon webUI，会重定向到zk选举出来的Master节点，以10.120.181.94为例: 12mesos webUI address: 10.120.181.94:5050marathon webUI address: 10.120.181.94:8080 部署Flink编写json格式的marathon应用描述文件flink-example.json: 12345678&#123; "id": "flink", "cmd": "/home/flink-1.2.0/bin/mesos-appmaster.sh -Dmesos.master=zk://10.120.177.85:2181,10.120.181.94:2181,10.120.180.209:2181/mesos -Dmesos.initial-tasks=3 -Dmesos.resourcemanager.tasks.cpus=1.0 -Dmesos.resourcemanager.tasks.mem=1024", "cpus": 1, "mem": 1024, "disk": 2048, "instances": 1&#125; 通过Marathon WebUI提交或者使用curl命令提交: 1curl -X POST http://10.120.181.94:8080/v2/apps -d @flink-example.json -H &quot;Content-type: application/json&quot;]]></content>
      <categories>
        <category>Flink部署</category>
      </categories>
      <tags>
        <tag>Mesos</tag>
        <tag>Marathon</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy flink on mesos marathon]]></title>
    <url>%2F2017%2F04%2F12%2Fdeploy-flink-on-mesos-marathon%2F</url>
    <content type="text"><![CDATA[Mesos部署1.下载安装从官网下载Mesos的rpm安装包（下载链接）并在所有集群节点中安装mesos 1rpm -ivh mesos-1.1.0*.rpm 注：可能会报libevent-devel依赖未安装，使用命令yum install libevent libevent-devel -y安装 2.集群部署 在/usr/etc/mesos目录下增加名为masters和slaves的文件，分别配上master节点和agent(worker)节点的ip列表，以换行分隔，以1个master节点2个agent节点为例： 在master节点所在机器的/usr/etc/mesos目录下，增加mesos-master-env.sh脚本（在相同目录下已存在模板），配置mesos工作目录环境变量export MESOS_work_dir=/xxx/xxx 在agent节点所在机器（在本例中，包括9.96.101.32和9.96.101.251两个节点）的/usr/etc/mesos目录下，增加mesos-agent-env.sh脚本（在相同目录下已存在模板），配置mesos master地址export MESOS_master=xx.xx.xx.xx:5050，和mesos工作目录环境变量export MESOS_work_dir=/xxx/xxx 运行/usr/sbin/mesos-start-cluster.sh启动集群 访问mesos web页面http://masterIP:5050可以查看mesos集群状态和agent列表等信息 3.部署Flink到Mesos 修改FLINK_HOME/conf/flink-conf.yaml，增加三个配置项：mesos.resourcemanager.tasks.container.type、mesos.master、mesos.initial-tasks，如下图所示，其中mesos.initial-tasks配置项表示taskmanager个数。 运行FLINK_HOME/bin/mesos-appmaster.sh向mesos注册schduler并启动jobmanager 查看mesos web页面，可以看到刚才启动的Flink集群在mesos中体现为1个Framework和2个Task（executor），每个java进程（mesos中的executor，flink中的taskmanager）对应一个sandbox 4.查看沙箱目录沙箱是每个mesos executor工作时的临时目录。存在于agent节点的工作目录下，工作目录的配置见上文。沙箱的目录结构在官方文档中描述如下： 1234567891011root (&apos;--work_dir&apos;)|-- slaves| |-- latest (symlink)| |-- &lt;agent ID&gt;| |-- frameworks| |-- &lt;framework ID&gt;| |-- executors| |-- &lt;executor ID&gt;| |-- runs| |-- latest (symlink)| |-- &lt;container ID&gt; (Sandbox!) 登陆到本例中task所在节点9.96.101.251查看两个task各自的sandbox目录，包含了进程的标准输出、错误、日志，以及flink上传的jar包、配置文件和shell脚本等，与yarn的container目录很类似。 沙箱中的文件包含如下三部分： mesos在启动executor的task前获取的文件（flink文件夹） executor的输出（stderr，stdout） executor创建的文件（flink-taskmanager.log等） Marathon部署1.简介1Marathon is a production-proven Apache Mesos framework for container orchestration. Marathon provides a REST API for starting, stopping, and scaling applications. 翻译一下，Marathon是一个在工业界广泛使用的Mesos框架，用于容器编排。Marathon提供了一套REST API用于任务的启动、停止和扩容。 Marathon框架通常用于管理长时间运行的任务。 2.下载安装从官网下载Marathon的rpm安装包（下载链接）并安装（推荐的安装方式） 1rpm -ivh marathon-1.4.0*.rpm 也可以从Marathon官网下载二进制的包（下载地址），使用$MARATHON_HOME/bin/start --master ip:port --zk zk://ip:port/marathon启动Marathon。 3.启动命令样例如下，需要首先启动mesos集群和zk服务，不在此赘述。 1/usr/bin/marathon run_jar --master 10.120.177.85:5050 --zk zk://10.120.177.85:2181/marathon 注：不同版本marathon在zk上的leader信息不兼容，需确保zk中/marathon目录未被不同版本的marathon服务使用过 可以使用/usr/bin/marathon run_jar --help查看其它配置项说明。 4.提交任务可以用两种方式提交应用，WebUI和curl。 打开Marathon WebUI（默认端口8080），点击Create Application，可以选在使用JSON模式描述任务信息，或者使用UI界面，本例中使用JSON方式，在cmd中写入运行的命令，点击右下角的Create Application提交。 使用curl方式提交命令格式如下： 1curl -X POST http://10.120.177.85:8080/v2/apps -d @basic-0.json -H &quot;Content-type: application/json&quot; 5.部署Flink只需要将cmd的内容修改为$FLINK_HOME/bin/mesos-appmaster.sh -Dmesos.master=$MESOS_MASTER_IP:5050即可在Mesos中部署Flink集群。 涉及Flink的配置项列表参见Flink官网，资源相关的配置项包括mesos.initial-tasks、mesos.resourcemanager.tasks.mem和mesos.resourcemanager.tasks.cpus，分别代表启用多少个TaskManager以及每个TaskManager分配多少内存和CPU资源。 Mesos的资源分配使用DRF算法，参考论文：Dominant Resource Fairness: Fair Allocation of Multiple Resource Types，解决多种资源类型(主要考虑CPU和内存)的系统的公平资源分配问题。 使用如下json文件启动flink应用： 12345678&#123; "id": "/flink/flink1", "cmd": "/home/flink-1.2.0/bin/mesos-appmaster.sh -Dmesos.master=10.120.177.85:5050 -Dmesos.initial-tasks=3 -Dmesos.resourcemanager.tasks.cpus=1.0 -Dmesos.resourcemanager.tasks.mem=1024 -Djobmanager.web.port=-1", "cpus": 1, "mem": 1024, "disk": 2048, "instances": 1&#125; 使用Marathon的应用扩容功能，将实例数改为3： 在Mesos页面中查看部署结果： Scale Application的过程中，因JobManager RPC端口冲突，会重试数次，最终三个JobManager部署到三个不同的节点上。 再增加一个Flink应用，因集群资源有限，只有三个节点，因此在运行参数中增加-Djobmanager.web.port=8082 -Djobmanager.rpc.port=6124，避免端口冲突。 12345678&#123; "id": "/flink/flink2", "cmd": "/home/flink-1.2.0/bin/mesos-appmaster.sh -Dmesos.master=10.120.177.85:5050 -Dmesos.initial-tasks=3 -Dmesos.resourcemanager.tasks.cpus=1.0 -Dmesos.resourcemanager.tasks.mem=1024 -Djobmanager.web.port=-1 -Djobmanager.rpc.port=6124", "cpus": 1, "mem": 1024, "disk": 2048, "instances": 1&#125; 部署结果如下: 6.遇到的问题 在Marathon中部署跨节点Flink集群时，部署失败 查看TaskManager日志，报Association failed with [akka.tcp://flink@szv1000265118:6123] Connection refused: szv1000265118/10.120.181.94:6123而JobManager所在节点hostname为SZV1000265118，怀疑与hostname大小写有关。 相关github PR，修改hostname后问题解决。 1hostnamectl set-hostname &lt;hostname&gt; 在Marathon中Destroy Application后JobManager退出，但是TaskManager要等数分钟后才退出 查看日志发现TaskManager向JobManager注册多次后超时关闭，查看代码发现MesosApplicationMasterRunner.java中有如下代码，TaskManager向JobManager注册的最大超时时间为5分钟，在代码中写死，无配置项。 1private static final FiniteDuration TASKMANAGER_REGISTRATION_TIMEOUT = new FiniteDuration(5, TimeUnit.MINUTES); systemd Mesos守护进程问题 在Mesos集群重启时，经常发现agent进程起不来，ps -ef | grep mesos查看系统进程时发现运行/usr/sbin/mesos-stop-cluster.sh一段时间后，会出现mesos-master和mesos-slave进程。 rpm方式安装mesos时，会向systemd注册mesos相关服务，导致mesos相关进程在被杀掉后周期重启，见下图： 使用systemctl命令停止mesos相关服务后再启动Mesos集群，问题解决。 1234systemctl stop mesos-mastersystemctl stop mesos-slavepkill mesos/usr/sbin/mesos-start-cluster.sh CentOS防火墙问题导致Mesos、Marathon网页无法访问 1systemctl stop firewalld]]></content>
      <categories>
        <category>Flink部署</category>
      </categories>
      <tags>
        <tag>Mesos</tag>
        <tag>Marathon</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tips in Functional Programming in Scala]]></title>
    <url>%2F2017%2F04%2F06%2Ffunctional-programming-in-scala-tips%2F</url>
    <content type="text"><![CDATA[update everyday(maybe weekly) partial application 123456789def compose[A,B,C](f: B =&gt; C, g: A =&gt; B): A =&gt; C = &#123; (a: A) =&gt; f(g(a))&#125;def curry[A,B,C](f: (A, B) =&gt; C): A =&gt; (B =&gt; C) = &#123; (a: A) =&gt; ((b: B) =&gt; f(a, b))&#125;def uncurry[A,B,C](f: A =&gt; B =&gt; C): (A, B) =&gt; C = &#123; (a: A, b: B) =&gt; f(a)(b)&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Libprocess concepts]]></title>
    <url>%2F2017%2F03%2F29%2Flibprocess-tips%2F</url>
    <content type="text"><![CDATA[先看官方定义： 1Library that provides an actor style message-passing programming model (in C++). mesos代码中大篇幅引用libprocess，其中定义了大量异步编程的原语，包括future、promise这些在c++11中也有的概念，onReady、onAny等回调注册接口，以及then串接异步调用等等强大功能。 future／promise 概念先看cplusplus.com官方定义： 12341. A future is an object that can retrieve a value from some provider object or function, properly synchronizing this access if in different threads.2. A promise is an object that can store a value of type T to be retreieved by a future object (possibly in another thread), offering a synchronization point. libprocess中实现了一套类似的future／promise（类比std::future／std::promise），其思想与c++很类似，future读，promise写，消费者持有future，生产者持有promise，在命名上别有意趣。 callbackFuture对象在函数返回值和入参间传递，存在多个消费者持有Future对象的多份拷贝的情况，因此Future使用一个共享指针成员维护唯一一份Data，在Data中记录Future状态和一系列callback，这些callback由onXXX接口注册，当生产者调用Promise的set等接口改变Future状态时将调用对应的callback。then也应用了callback机制。 then先看示例代码： 12345Future&lt;string&gt; S = readyFuture() .then(lambda::bind(&amp;second, lambda::_1)) .then(lambda::bind(&amp;third, lambda::_1));string s = S.get(); 等同于： 12345Future&lt;bool&gt; A = oneFuture();Future&lt;int&gt; B = second(A.get());Future&lt;string&gt; S = third(B.get());string s = S.get();]]></content>
      <categories>
        <category>Mesos</category>
      </categories>
      <tags>
        <tag>libprocess</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mesos Containerizer]]></title>
    <url>%2F2017%2F03%2F24%2Fmesos-containerizer-tips%2F</url>
    <content type="text"><![CDATA[Mesos Containerizer是mesos agent的关键部件，提供容器化所需的服务。 位于agent和容器之间 启动、更新和销毁容器 提供容器间的隔离 上报容器状态 当前mesos支持以多种方式提供容器服务：Docker Containerizer、Mesos Containerizer以及两者混合。Mesos Containerizer更加稳定。 Mesos Containerizer主要包括以下三个组件： Launcher，负责启动和销毁容器进程 Isolator，最主要的功能组件，通过cgroup、namespace实现容器间资源隔离 Provisioner，提供容器镜像支持 容器启动的主要步骤如下： Isolator准备 包含创建cgroup目录和初始化cgroup subsystem相关参数等操作。举一个cgroup subsystem为memory的例子，经过CgroupsIsolatorProcess::prepare的调用过程后，新生成了如下cgroup目录，并设置相关初始化参数 通过Launcher启动容器进程 Launcher的具体实现LinuxLauncher通过libprocess的actor模型调用LinuxLauncherProcess::fork创建容器子进程。主要包括创建subsystem为freezer的cgroup，并将子进程pid放入其中，如下图所示：放入freezer cgroup的进程会暂停，直到隔离和fetch完成后，在exec时通过信号将子进程唤醒。 隔离容器进程 Isolator的具体实现有很多，主要包括cgroup、network cni等资源隔离接口，其中cgroup的隔离的过程就是将进程pid放入对应的cgroup中。具体实现在CgroupsIsolatorProcess::isolate fetch容器资源 fetch是一种在容器任务准备时将容器资源下载到沙箱目录的机制，为后续的exec过程做准备。 执行容器进程 之前fork出来的容器子进程被唤醒。]]></content>
      <categories>
        <category>Mesos</category>
      </categories>
      <tags>
        <tag>Mesos</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F03%2F24%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
